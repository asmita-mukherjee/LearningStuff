{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/asmitamukh/basic-bpe?scriptVersionId=145723510\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## References\n\n>https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\n\n>https://huggingface.co/transformers/v3.2.0/_modules/transformers/tokenization_gpt2.html","metadata":{"id":"A7LNYZK4iF-j"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"HLHu6mskaAA7"}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm.notebook import tqdm\nimport logging\nimport sys\nimport nltk\nfrom collections import defaultdict","metadata":{"id":"jV3pk6WuaBhX","execution":{"iopub.status.busy":"2023-10-08T08:27:38.450209Z","iopub.execute_input":"2023-10-08T08:27:38.450719Z","iopub.status.idle":"2023-10-08T08:27:40.378478Z","shell.execute_reply.started":"2023-10-08T08:27:38.450675Z","shell.execute_reply":"2023-10-08T08:27:40.37762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"id":"qQwoYHOdekgW","execution":{"iopub.status.busy":"2023-10-08T08:27:40.379891Z","iopub.execute_input":"2023-10-08T08:27:40.38096Z","iopub.status.idle":"2023-10-08T08:27:40.385161Z","shell.execute_reply.started":"2023-10-08T08:27:40.38093Z","shell.execute_reply":"2023-10-08T08:27:40.383976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nnltk.download('all')","metadata":{"id":"za8-rKg8auIT","outputId":"4548513e-4705-4ade-d956-cdeab276c460","execution":{"iopub.status.busy":"2023-10-08T08:27:40.386206Z","iopub.execute_input":"2023-10-08T08:27:40.386499Z","iopub.status.idle":"2023-10-08T08:28:10.25283Z","shell.execute_reply.started":"2023-10-08T08:27:40.386474Z","shell.execute_reply":"2023-10-08T08:28:10.251721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"id":"nylmk79mA0dQ","execution":{"iopub.status.busy":"2023-10-08T08:28:10.257153Z","iopub.execute_input":"2023-10-08T08:28:10.25792Z","iopub.status.idle":"2023-10-08T08:28:10.264262Z","shell.execute_reply.started":"2023-10-08T08:28:10.257881Z","shell.execute_reply":"2023-10-08T08:28:10.262898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nlogging.basicConfig(filename=\"logs.log\",level=logging.DEBUG,format=\"%(asctime)s %(message)s\",force=True)","metadata":{"id":"-7w_T7RCBW6o","execution":{"iopub.status.busy":"2023-10-08T08:28:10.265805Z","iopub.execute_input":"2023-10-08T08:28:10.266583Z","iopub.status.idle":"2023-10-08T08:28:14.205172Z","shell.execute_reply.started":"2023-10-08T08:28:10.266543Z","shell.execute_reply":"2023-10-08T08:28:14.20386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the data\n> Download the 1st paraquet of tiny stories data","metadata":{"id":"Rzxo4NVBZnuE"}},{"cell_type":"code","source":"urls = [\"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet\",\n        \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/refs%2Fconvert%2Fparquet/default/train/0001.parquet\",\n        \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/refs%2Fconvert%2Fparquet/default/train/0002.parquet\",\n        \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/refs%2Fconvert%2Fparquet/default/train/0003.parquet\"]","metadata":{"id":"dlFGZf4jZNwy","execution":{"iopub.status.busy":"2023-10-08T08:28:14.207557Z","iopub.execute_input":"2023-10-08T08:28:14.208078Z","iopub.status.idle":"2023-10-08T08:28:14.27044Z","shell.execute_reply.started":"2023-10-08T08:28:14.20803Z","shell.execute_reply":"2023-10-08T08:28:14.2696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urls = urls[:1]","metadata":{"id":"INv1P09RZ1GD","execution":{"iopub.status.busy":"2023-10-08T08:28:14.272515Z","iopub.execute_input":"2023-10-08T08:28:14.273194Z","iopub.status.idle":"2023-10-08T08:28:14.335455Z","shell.execute_reply.started":"2023-10-08T08:28:14.273162Z","shell.execute_reply":"2023-10-08T08:28:14.334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_downloaded_urls = []\nfor url in tqdm(urls):\n        df = pd.read_parquet(url)\n        logging.info(f\"downloaded file of size {sys.getsizeof(df)}\")\n        list_of_downloaded_urls.append(df)\nfinal_df = pd.concat(list_of_downloaded_urls)\n","metadata":{"id":"g9Jp0tDrZ20T","outputId":"e19d5cfc-f117-4efc-eade-4008283d551f","execution":{"iopub.status.busy":"2023-10-08T08:28:14.337265Z","iopub.execute_input":"2023-10-08T08:28:14.338162Z","iopub.status.idle":"2023-10-08T08:28:24.915086Z","shell.execute_reply.started":"2023-10-08T08:28:14.338119Z","shell.execute_reply":"2023-10-08T08:28:24.913761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.info()","metadata":{"id":"rV-Ge_I0aHJH","outputId":"592b935e-49a8-483f-af32-ab0bc23709d3","execution":{"iopub.status.busy":"2023-10-08T08:28:24.916379Z","iopub.execute_input":"2023-10-08T08:28:24.916706Z","iopub.status.idle":"2023-10-08T08:28:25.010417Z","shell.execute_reply.started":"2023-10-08T08:28:24.916669Z","shell.execute_reply":"2023-10-08T08:28:25.009146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cutting down the data\nfinal_df = final_df.iloc[:30000]","metadata":{"execution":{"iopub.status.busy":"2023-10-08T08:28:25.013834Z","iopub.execute_input":"2023-10-08T08:28:25.01416Z","iopub.status.idle":"2023-10-08T08:28:25.018823Z","shell.execute_reply.started":"2023-10-08T08:28:25.01413Z","shell.execute_reply":"2023-10-08T08:28:25.017888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.head()","metadata":{"id":"j7lxQoriac-q","outputId":"c358a3b3-10c9-4c90-c18e-799d02129f1a","execution":{"iopub.status.busy":"2023-10-08T08:28:25.02002Z","iopub.execute_input":"2023-10-08T08:28:25.020295Z","iopub.status.idle":"2023-10-08T08:28:25.048167Z","shell.execute_reply.started":"2023-10-08T08:28:25.020272Z","shell.execute_reply":"2023-10-08T08:28:25.047055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_corpus,test_corpus = train_test_split(final_df,test_size=0.005,random_state=2023)","metadata":{"id":"0LBh1zOHeeEG","execution":{"iopub.status.busy":"2023-10-08T08:28:25.049826Z","iopub.execute_input":"2023-10-08T08:28:25.050127Z","iopub.status.idle":"2023-10-08T08:28:25.061073Z","shell.execute_reply.started":"2023-10-08T08:28:25.050102Z","shell.execute_reply":"2023-10-08T08:28:25.059929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_corpus),len(test_corpus)","metadata":{"id":"rM3_q8tNe7vA","outputId":"787f56d4-4404-4331-939b-ea1a827624ee","execution":{"iopub.status.busy":"2023-10-08T08:28:25.062979Z","iopub.execute_input":"2023-10-08T08:28:25.063413Z","iopub.status.idle":"2023-10-08T08:28:25.070779Z","shell.execute_reply.started":"2023-10-08T08:28:25.063379Z","shell.execute_reply":"2023-10-08T08:28:25.069896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BPE of the dataset\n\n> Word Tokenize\n\n>Splitting into individual chars\n\n>Merging to build a vocab","metadata":{"id":"-rV9m-p-ahXa"}},{"cell_type":"code","source":"def train_tokenize(texts,vocab_length,min_freq=5):\n\n  vocab = set()\n  words = []\n  logging.info(\"Pre tokenizing\")\n  for text in texts:\n    text = text.lower()\n    words.extend(nltk.word_tokenize(text))\n\n  word_count_dict = defaultdict(int)\n  def rem_word(word):\n        rem_words = [w for w in words if w!=word]\n        return rem_words\n  logging.info(\"Counting freq of words\")\n  for word in words:\n    word_count_dict[word] = word_count_dict[word] + 1\n  rare_word_threshold = min_freq\n  logging.info(f\"Total nos of words in corpus {len(words)}\")\n  for word in word_count_dict.keys():\n    if word_count_dict[word] <rare_word_threshold:\n        words = rem_word(word)\n        logging.info(f\"Removed word {word} due to low frequency\")\n        logging.info(f\"Nos of words in corpus after deleting {len(words)}\")\n  word_dict = dict()\n\n  for word in words:\n    word_dict[word] = []\n    for char in word:\n      word_dict[word].append(char)\n  logging.info(\"Building the base vocab\")\n  ##vocab\n  for word in word_dict.keys():\n    vocab.update(word_dict[word])\n\n  #merge rule dictonary\n  merge_rules = dict()\n  logging.info(\"Starting with building merge rules\")\n  while(len(vocab)<=vocab_length):\n    logging.info(\"vocab updation\")\n    #find pair_freq\n    pair_freq = defaultdict(int)\n    for word in word_dict.keys():\n      w_count = word_count_dict[word]\n      tokens = word_dict[word]\n      for token_idx in range(len(tokens)-1):\n        pair = (tokens[token_idx],tokens[token_idx+1])\n        pair_freq[pair] = pair_freq[pair] + w_count\n\n    #find the max freq pair\n    max_freq = 0\n    max_pair = ()\n    for pair in pair_freq.keys():\n      if max_freq<pair_freq[pair]:\n        max_freq = pair_freq[pair]\n        max_pair = pair\n    if len(max_pair) == 0:\n      logging.warning(f\"Vocabulary cannot be extended further. Exiting . Max vocab size achieved i.e all tokens in the corpus is captured {len(vocab)}\")\n      break\n\n    ##TODO What to do when max_pair = 0 i.e all pair_freq is 0\n\n    ##merge\n    for word in word_dict.keys():\n      tokens = word_dict[word]\n      for tok_idx in range(len(tokens)-1):\n        pair = (tokens[tok_idx],tokens[tok_idx+1])\n        if pair==max_pair:\n          merge_rules[pair] = pair[0]+pair[1]\n          word_dict[word] = word_dict[word][:tok_idx]+[tokens[tok_idx]+tokens[tok_idx+1]]+word_dict[word][tok_idx+2:]\n\n    vocab.add(max_pair[0]+max_pair[1])\n  logging.info(f\"Vocab updated \\n {vocab}\")\n\n  return vocab,merge_rules\n","metadata":{"id":"Q0qdWJfVnjmP","execution":{"iopub.status.busy":"2023-10-08T08:28:25.07216Z","iopub.execute_input":"2023-10-08T08:28:25.072582Z","iopub.status.idle":"2023-10-08T08:28:25.087089Z","shell.execute_reply.started":"2023-10-08T08:28:25.072544Z","shell.execute_reply":"2023-10-08T08:28:25.086133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(text):\n  #word tokenize the\n  text = text.lower()\n  words = nltk.word_tokenize(text)\n  #split each word into its charecters\n  word_dict = dict()\n  for word in words:\n    word_dict[word] = []\n    for char in word:\n      word_dict[word].append(char)\n\n  #tokenize each word as per the merge rules\n  #for each merge rule iterate through each word to see what tokens can be formed of the word\n  for pair,merge in merge_rules.items():\n    for word in word_dict.keys():\n      tokens = word_dict[word]\n      i=0\n      while(i<len(tokens)-1):\n        pair_word = (tokens[i],tokens[i+1])\n        if pair_word == pair:\n          tokens = tokens[:i] + [pair_word[0]+pair_word[1]] + tokens[i+2:]\n          word_dict[word] = tokens\n        else:\n          i = i + 1\n  print(word_dict)","metadata":{"id":"YaLv7DJUn7t4","execution":{"iopub.status.busy":"2023-10-08T08:28:25.088497Z","iopub.execute_input":"2023-10-08T08:28:25.089059Z","iopub.status.idle":"2023-10-08T08:28:25.106187Z","shell.execute_reply.started":"2023-10-08T08:28:25.089032Z","shell.execute_reply":"2023-10-08T08:28:25.105121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_corpus = list(train_corpus[\"text\"])","metadata":{"id":"wmfnWQWdV4FH","execution":{"iopub.status.busy":"2023-10-08T08:28:25.107698Z","iopub.execute_input":"2023-10-08T08:28:25.108123Z","iopub.status.idle":"2023-10-08T08:28:25.13098Z","shell.execute_reply.started":"2023-10-08T08:28:25.108096Z","shell.execute_reply":"2023-10-08T08:28:25.129899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## finding the unique nos of words\nuq_words = set()\n\nfor sen in tqdm(train_corpus):\n    words = nltk.word_tokenize(sen)\n    for word in words:\n        uq_words.add(word)\n\n","metadata":{"id":"cRrE8rs4sG7P","outputId":"e8cdfda9-eee9-40e2-95f7-691a72f0491c","execution":{"iopub.status.busy":"2023-10-08T08:28:25.132312Z","iopub.execute_input":"2023-10-08T08:28:25.13281Z","iopub.status.idle":"2023-10-08T08:29:24.907159Z","shell.execute_reply.started":"2023-10-08T08:28:25.132769Z","shell.execute_reply":"2023-10-08T08:29:24.90595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(uq_words))","metadata":{"execution":{"iopub.status.busy":"2023-10-08T08:29:24.908648Z","iopub.execute_input":"2023-10-08T08:29:24.908967Z","iopub.status.idle":"2023-10-08T08:29:24.9144Z","shell.execute_reply.started":"2023-10-08T08:29:24.908941Z","shell.execute_reply":"2023-10-08T08:29:24.913219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the number of unique words is around 16000, hence keeping the vocab size around 17000 in order to capture a good number of tokens","metadata":{}},{"cell_type":"code","source":"vocab_size = 17000","metadata":{"execution":{"iopub.status.busy":"2023-10-08T08:29:24.915869Z","iopub.execute_input":"2023-10-08T08:29:24.916159Z","iopub.status.idle":"2023-10-08T08:29:24.931188Z","shell.execute_reply.started":"2023-10-08T08:29:24.916135Z","shell.execute_reply":"2023-10-08T08:29:24.929973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvocab,merge_rules = train_tokenize(train_corpus,vocab_size)","metadata":{"id":"Pk6K-wwn5SIq","outputId":"6014dbbb-1a31-4684-abb7-68088daba0d1","execution":{"iopub.status.busy":"2023-10-08T08:29:24.932825Z","iopub.execute_input":"2023-10-08T08:29:24.933176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(merge_rules),len(vocab)","metadata":{"id":"FdWbxzJpcEd1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"merge_rules.pkl\",\"wb\") as f:\n    pickle.dump(merge_rules,f)\n\nwith open(\"vocab.pkl\",\"wb\") as f:\n    pickle.dump(vocab,f)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test time","metadata":{"id":"zubcvuAKnM2e"}},{"cell_type":"code","source":"# @title Default title text\ntext = \"This is not a token.\"","metadata":{"id":"JRtOGt055VNN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenize(text)","metadata":{"id":"JN0rft89wOwv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"R0D2xovueD9C"},"execution_count":null,"outputs":[]}]}