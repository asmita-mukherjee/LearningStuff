{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd41c33b",
   "metadata": {
    "papermill": {
     "duration": 0.005111,
     "end_time": "2023-10-14T09:49:34.818628",
     "exception": false,
     "start_time": "2023-10-14T09:49:34.813517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec68b396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:49:34.830053Z",
     "iopub.status.busy": "2023-10-14T09:49:34.829346Z",
     "iopub.status.idle": "2023-10-14T09:49:34.837309Z",
     "shell.execute_reply": "2023-10-14T09:49:34.836564Z"
    },
    "papermill": {
     "duration": 0.015924,
     "end_time": "2023-10-14T09:49:34.839370",
     "exception": false,
     "start_time": "2023-10-14T09:49:34.823446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94218cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:49:34.850581Z",
     "iopub.status.busy": "2023-10-14T09:49:34.850219Z",
     "iopub.status.idle": "2023-10-14T09:49:34.853990Z",
     "shell.execute_reply": "2023-10-14T09:49:34.853226Z"
    },
    "papermill": {
     "duration": 0.011628,
     "end_time": "2023-10-14T09:49:34.855901",
     "exception": false,
     "start_time": "2023-10-14T09:49:34.844273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e28089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:49:34.867313Z",
     "iopub.status.busy": "2023-10-14T09:49:34.866611Z",
     "iopub.status.idle": "2023-10-14T09:49:38.426957Z",
     "shell.execute_reply": "2023-10-14T09:49:38.425810Z"
    },
    "papermill": {
     "duration": 3.568878,
     "end_time": "2023-10-14T09:49:38.429431",
     "exception": false,
     "start_time": "2023-10-14T09:49:34.860553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54abf124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:49:38.440852Z",
     "iopub.status.busy": "2023-10-14T09:49:38.440381Z",
     "iopub.status.idle": "2023-10-14T09:50:11.733333Z",
     "shell.execute_reply": "2023-10-14T09:50:11.732017Z"
    },
    "papermill": {
     "duration": 33.301219,
     "end_time": "2023-10-14T09:50:11.735557",
     "exception": false,
     "start_time": "2023-10-14T09:49:38.434338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e8c4ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.751900Z",
     "iopub.status.busy": "2023-10-14T09:50:11.751435Z",
     "iopub.status.idle": "2023-10-14T09:50:11.757436Z",
     "shell.execute_reply": "2023-10-14T09:50:11.756170Z"
    },
    "papermill": {
     "duration": 0.016906,
     "end_time": "2023-10-14T09:50:11.759699",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.742793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"logs.log\",format=\"%(asctime)s - %(message)s\",level=logging.DEBUG,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85e0363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.776094Z",
     "iopub.status.busy": "2023-10-14T09:50:11.775634Z",
     "iopub.status.idle": "2023-10-14T09:50:11.780551Z",
     "shell.execute_reply": "2023-10-14T09:50:11.779191Z"
    },
    "papermill": {
     "duration": 0.015593,
     "end_time": "2023-10-14T09:50:11.782590",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.766997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sen1 = \"This is an apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e9a76",
   "metadata": {
    "papermill": {
     "duration": 0.007848,
     "end_time": "2023-10-14T09:50:11.797747",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.789899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get the vector for a sentence\n",
    "\n",
    "> Tokenize \n",
    "> For each token get the index from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a823a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.817342Z",
     "iopub.status.busy": "2023-10-14T09:50:11.816617Z",
     "iopub.status.idle": "2023-10-14T09:50:11.849804Z",
     "shell.execute_reply": "2023-10-14T09:50:11.848420Z"
    },
    "papermill": {
     "duration": 0.045025,
     "end_time": "2023-10-14T09:50:11.852843",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.807818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the vocab and merge rules\n",
    "\n",
    "with open(\"/kaggle/input/tinystories-custom-bpe/merge_rules.pkl\",\"rb\") as f:\n",
    "    merge_rules = pickle.load(f)\n",
    "    \n",
    "with open(\"/kaggle/input/tinystories-custom-bpe/vocab.pkl\",\"rb\") as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b9333c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.875196Z",
     "iopub.status.busy": "2023-10-14T09:50:11.874387Z",
     "iopub.status.idle": "2023-10-14T09:50:11.881427Z",
     "shell.execute_reply": "2023-10-14T09:50:11.880057Z"
    },
    "papermill": {
     "duration": 0.019862,
     "end_time": "2023-10-14T09:50:11.884259",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.864397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9786, 9840)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merge_rules),len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73939de4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.905656Z",
     "iopub.status.busy": "2023-10-14T09:50:11.904439Z",
     "iopub.status.idle": "2023-10-14T09:50:11.918165Z",
     "shell.execute_reply": "2023-10-14T09:50:11.917183Z"
    },
    "papermill": {
     "duration": 0.028979,
     "end_time": "2023-10-14T09:50:11.921070",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.892091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text,merge_rules):\n",
    "    '''Return tokens from the text'''\n",
    "    logging.info(f\"Starting tokenization for the text -> {text}\")\n",
    "    text = text.lower() # our vocabulary is uncased.\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    ##preparing dict where word is the key and the tokens is its value\n",
    "    word_dict = dict()\n",
    "    for word in words:\n",
    "        word_dict[word] = []\n",
    "        for char in word:\n",
    "            word_dict[word].append(char)\n",
    "            \n",
    "    logging.info(\"Starting to iterate through each merge rule\")\n",
    "    \n",
    "    for merge_rule in merge_rules.keys():\n",
    "        #go through each merge rule and tokenize each word from left to right\n",
    "        for word in word_dict.keys():\n",
    "            tokens = word_dict[word]\n",
    "            idx=0\n",
    "            while(idx<len(tokens)-1):\n",
    "                pair = (tokens[idx],tokens[idx+1])\n",
    "                if pair==merge_rule:\n",
    "                    logging.info(f\"merging for word {word}\")\n",
    "                    #merge the token of the word as per the merge rule\n",
    "                    #if merged we dont increase the index of the tokens because the next pair should include the new pair and the next char\n",
    "                    tokens = tokens[:idx]+[tokens[idx]+tokens[idx+1]]+tokens[idx+2:]\n",
    "                    word_dict[word] = tokens\n",
    "                else:\n",
    "                    #if not merged then we slide the window over\n",
    "                    idx = idx+1\n",
    "        logging.info(f\"After merge {word_dict}\")\n",
    "    return word_dict    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351da7b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.937461Z",
     "iopub.status.busy": "2023-10-14T09:50:11.936945Z",
     "iopub.status.idle": "2023-10-14T09:50:11.944253Z",
     "shell.execute_reply": "2023-10-14T09:50:11.942823Z"
    },
    "papermill": {
     "duration": 0.017887,
     "end_time": "2023-10-14T09:50:11.946349",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.928462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'th'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_rules[('t', 'h')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2346cd0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:11.963392Z",
     "iopub.status.busy": "2023-10-14T09:50:11.962094Z",
     "iopub.status.idle": "2023-10-14T09:50:12.267564Z",
     "shell.execute_reply": "2023-10-14T09:50:12.266283Z"
    },
    "papermill": {
     "duration": 0.316467,
     "end_time": "2023-10-14T09:50:12.270008",
     "exception": false,
     "start_time": "2023-10-14T09:50:11.953541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': ['this'], 'is': ['is'], 'an': ['an'], 'apple': ['apple']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sen1,merge_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351b572d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:12.287558Z",
     "iopub.status.busy": "2023-10-14T09:50:12.286773Z",
     "iopub.status.idle": "2023-10-14T09:50:12.293375Z",
     "shell.execute_reply": "2023-10-14T09:50:12.292113Z"
    },
    "papermill": {
     "duration": 0.018581,
     "end_time": "2023-10-14T09:50:12.296004",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.277423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sen_to_tensor(sen,vocab,merge_rules):\n",
    "    '''Each sen will be converted to a vector where each token will be mapped to its index in vocab'''\n",
    "    sen = sen.lower()\n",
    "    token_dict = tokenize(sen,merge_rules)\n",
    "    words = nltk.word_tokenize(sen)\n",
    "    vocab = list(vocab)\n",
    "    list_of_tokens = []\n",
    "    for word in words:\n",
    "        list_of_tokens.extend(token_dict[word])\n",
    "    \n",
    "    #find the index of the token in the vocab\n",
    "    tensor = np.empty(len(list_of_tokens),dtype=int)\n",
    "    for idx,token in enumerate(list_of_tokens):\n",
    "        tensor[idx] = vocab.index(token)\n",
    "    return tensor\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8415ba11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:12.313329Z",
     "iopub.status.busy": "2023-10-14T09:50:12.312868Z",
     "iopub.status.idle": "2023-10-14T09:50:12.607754Z",
     "shell.execute_reply": "2023-10-14T09:50:12.606825Z"
    },
    "papermill": {
     "duration": 0.306396,
     "end_time": "2023-10-14T09:50:12.610095",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.303699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4290, 1064, 9373, 5047])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_to_tensor(sen1,vocab,merge_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d46e9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:12.628161Z",
     "iopub.status.busy": "2023-10-14T09:50:12.627467Z",
     "iopub.status.idle": "2023-10-14T09:50:12.633305Z",
     "shell.execute_reply": "2023-10-14T09:50:12.632511Z"
    },
    "papermill": {
     "duration": 0.017926,
     "end_time": "2023-10-14T09:50:12.635543",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.617617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_embed_sen(sen_tensor,n=100,d=4):\n",
    "    '''Given a sen tensor return its pos embed'''\n",
    "    seq_len = len(sen_tensor)\n",
    "    pos_embed_matrix = np.zeros((seq_len,d))\n",
    "    \n",
    "    for k in range(len(sen_tensor)):\n",
    "        for i in range(0,int(d/2)):\n",
    "            denom = np.power(n,((2*i)/d))\n",
    "            pos_embed_matrix[k,2*i] = np.sin(k/denom)#for even embed dim\n",
    "            pos_embed_matrix[k,2*i+1] = np.cos(k/denom)#for odd embed dim\n",
    "    \n",
    "    return pos_embed_matrix\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5bf5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:12.652752Z",
     "iopub.status.busy": "2023-10-14T09:50:12.652034Z",
     "iopub.status.idle": "2023-10-14T09:50:12.948743Z",
     "shell.execute_reply": "2023-10-14T09:50:12.947455Z"
    },
    "papermill": {
     "duration": 0.308235,
     "end_time": "2023-10-14T09:50:12.951311",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.643076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.09983342,  0.99500417],\n",
       "       [ 0.90929743, -0.41614684,  0.19866933,  0.98006658],\n",
       "       [ 0.14112001, -0.9899925 ,  0.29552021,  0.95533649]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor= sen_to_tensor(sen1,vocab,merge_rules)\n",
    "pos_embed = pos_embed_sen(tensor)\n",
    "pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60860e",
   "metadata": {
    "papermill": {
     "duration": 0.007367,
     "end_time": "2023-10-14T09:50:12.966789",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.959422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merge with Token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc95f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:12.984522Z",
     "iopub.status.busy": "2023-10-14T09:50:12.983321Z",
     "iopub.status.idle": "2023-10-14T09:50:12.989749Z",
     "shell.execute_reply": "2023-10-14T09:50:12.988491Z"
    },
    "papermill": {
     "duration": 0.017581,
     "end_time": "2023-10-14T09:50:12.991973",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.974392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#consider the token embeddings to be a random numpy matrix\n",
    "def token_embed_sen(tensor,vocab_len,d=4):\n",
    "    \n",
    "    #torch will create a looup table of dim vocab_len*embedding_dim\n",
    "    #at the time of output it will look up the embedding and return\n",
    "    embed_layer = nn.Embedding(vocab_len,d)\n",
    "    print(embed_layer.weight.shape)\n",
    "  \n",
    "    output = embed_layer(torch.tensor(tensor))\n",
    "    \n",
    "    #to only return the matrix without the grad\n",
    "    return output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "784b174e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:13.010303Z",
     "iopub.status.busy": "2023-10-14T09:50:13.009847Z",
     "iopub.status.idle": "2023-10-14T09:50:13.374749Z",
     "shell.execute_reply": "2023-10-14T09:50:13.373371Z"
    },
    "papermill": {
     "duration": 0.377358,
     "end_time": "2023-10-14T09:50:13.376976",
     "exception": false,
     "start_time": "2023-10-14T09:50:12.999618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9840, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.31746653,  0.14460814,  0.6741513 , -1.1949483 ],\n",
       "       [ 0.30879948, -0.8313213 ,  0.6578424 ,  1.3326224 ],\n",
       "       [ 0.00236675, -0.5804003 ,  1.2028182 ,  1.8941643 ],\n",
       "       [ 0.21328416, -0.9833757 , -0.5708011 ,  1.4548188 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_of_sen = sen_to_tensor(sen1,vocab,merge_rules)\n",
    "token_embed = token_embed_sen(tensor,len(vocab))\n",
    "token_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4c1ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:13.394632Z",
     "iopub.status.busy": "2023-10-14T09:50:13.394173Z",
     "iopub.status.idle": "2023-10-14T09:50:13.401043Z",
     "shell.execute_reply": "2023-10-14T09:50:13.399817Z"
    },
    "papermill": {
     "duration": 0.018395,
     "end_time": "2023-10-14T09:50:13.403160",
     "exception": false,
     "start_time": "2023-10-14T09:50:13.384765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4, 4))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(token_embed),np.shape(pos_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb0a9807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T09:50:13.421149Z",
     "iopub.status.busy": "2023-10-14T09:50:13.420468Z",
     "iopub.status.idle": "2023-10-14T09:50:13.426870Z",
     "shell.execute_reply": "2023-10-14T09:50:13.425656Z"
    },
    "papermill": {
     "duration": 0.018273,
     "end_time": "2023-10-14T09:50:13.429321",
     "exception": false,
     "start_time": "2023-10-14T09:50:13.411048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets add the pos and token embed to get the final embedding matrix\n",
    "final_embed = np.add(token_embed,pos_embed)\n",
    "\n",
    "final_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b8d86",
   "metadata": {
    "papermill": {
     "duration": 0.007613,
     "end_time": "2023-10-14T09:50:13.444858",
     "exception": false,
     "start_time": "2023-10-14T09:50:13.437245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.144233,
   "end_time": "2023-10-14T09:50:14.880050",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-14T09:49:31.735817",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
