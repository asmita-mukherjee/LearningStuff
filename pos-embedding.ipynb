{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c933bc0",
   "metadata": {
    "papermill": {
     "duration": 0.004176,
     "end_time": "2023-10-11T17:12:22.383433",
     "exception": false,
     "start_time": "2023-10-11T17:12:22.379257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8270fa89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:22.395200Z",
     "iopub.status.busy": "2023-10-11T17:12:22.394790Z",
     "iopub.status.idle": "2023-10-11T17:12:22.403618Z",
     "shell.execute_reply": "2023-10-11T17:12:22.402742Z"
    },
    "papermill": {
     "duration": 0.015885,
     "end_time": "2023-10-11T17:12:22.405499",
     "exception": false,
     "start_time": "2023-10-11T17:12:22.389614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1e7576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:22.415281Z",
     "iopub.status.busy": "2023-10-11T17:12:22.414958Z",
     "iopub.status.idle": "2023-10-11T17:12:22.419559Z",
     "shell.execute_reply": "2023-10-11T17:12:22.418564Z"
    },
    "papermill": {
     "duration": 0.011901,
     "end_time": "2023-10-11T17:12:22.421566",
     "exception": false,
     "start_time": "2023-10-11T17:12:22.409665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9f321a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:22.431347Z",
     "iopub.status.busy": "2023-10-11T17:12:22.430344Z",
     "iopub.status.idle": "2023-10-11T17:12:44.345732Z",
     "shell.execute_reply": "2023-10-11T17:12:44.344483Z"
    },
    "papermill": {
     "duration": 21.922735,
     "end_time": "2023-10-11T17:12:44.348210",
     "exception": false,
     "start_time": "2023-10-11T17:12:22.425475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76bc265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.361902Z",
     "iopub.status.busy": "2023-10-11T17:12:44.361400Z",
     "iopub.status.idle": "2023-10-11T17:12:44.366941Z",
     "shell.execute_reply": "2023-10-11T17:12:44.365834Z"
    },
    "papermill": {
     "duration": 0.014874,
     "end_time": "2023-10-11T17:12:44.369170",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.354296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"logs.log\",format=\"%(asctime)s - %(message)s\",level=logging.DEBUG,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982922bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.382621Z",
     "iopub.status.busy": "2023-10-11T17:12:44.382188Z",
     "iopub.status.idle": "2023-10-11T17:12:44.387049Z",
     "shell.execute_reply": "2023-10-11T17:12:44.385943Z"
    },
    "papermill": {
     "duration": 0.014058,
     "end_time": "2023-10-11T17:12:44.389190",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.375132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sen1 = \"This is an apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438e6f1",
   "metadata": {
    "papermill": {
     "duration": 0.005431,
     "end_time": "2023-10-11T17:12:44.400569",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.395138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get the vector for a sentence\n",
    "\n",
    "> Tokenize \n",
    "> For each token get the index from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810b37f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.413571Z",
     "iopub.status.busy": "2023-10-11T17:12:44.413220Z",
     "iopub.status.idle": "2023-10-11T17:12:44.450995Z",
     "shell.execute_reply": "2023-10-11T17:12:44.449924Z"
    },
    "papermill": {
     "duration": 0.047271,
     "end_time": "2023-10-11T17:12:44.453535",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.406264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the vocab and merge rules\n",
    "\n",
    "with open(\"/kaggle/input/tinystories-custom-bpe/merge_rules.pkl\",\"rb\") as f:\n",
    "    merge_rules = pickle.load(f)\n",
    "    \n",
    "with open(\"/kaggle/input/tinystories-custom-bpe/vocab.pkl\",\"rb\") as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7212e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.467206Z",
     "iopub.status.busy": "2023-10-11T17:12:44.466834Z",
     "iopub.status.idle": "2023-10-11T17:12:44.472900Z",
     "shell.execute_reply": "2023-10-11T17:12:44.471905Z"
    },
    "papermill": {
     "duration": 0.015053,
     "end_time": "2023-10-11T17:12:44.474693",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.459640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9786, 9840)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merge_rules),len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc983f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.489389Z",
     "iopub.status.busy": "2023-10-11T17:12:44.488910Z",
     "iopub.status.idle": "2023-10-11T17:12:44.499074Z",
     "shell.execute_reply": "2023-10-11T17:12:44.498123Z"
    },
    "papermill": {
     "duration": 0.020205,
     "end_time": "2023-10-11T17:12:44.501645",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.481440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text,merge_rules):\n",
    "    '''Return tokens from the text'''\n",
    "    logging.info(f\"Starting tokenization for the text -> {text}\")\n",
    "    text = text.lower() # our vocabulary is uncased.\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    ##preparing dict where word is the key and the tokens is its value\n",
    "    word_dict = dict()\n",
    "    for word in words:\n",
    "        word_dict[word] = []\n",
    "        for char in word:\n",
    "            word_dict[word].append(char)\n",
    "            \n",
    "    logging.info(\"Starting to iterate through each merge rule\")\n",
    "    \n",
    "    for merge_rule in merge_rules.keys():\n",
    "        #go through each merge rule and tokenize each word from left to right\n",
    "        for word in word_dict.keys():\n",
    "            tokens = word_dict[word]\n",
    "            idx=0\n",
    "            while(idx<len(tokens)-1):\n",
    "                pair = (tokens[idx],tokens[idx+1])\n",
    "                if pair==merge_rule:\n",
    "                    logging.info(f\"merging for word {word}\")\n",
    "                    #merge the token of the word as per the merge rule\n",
    "                    #if merged we dont increase the index of the tokens because the next pair should include the new pair and the next char\n",
    "                    tokens = tokens[:idx]+[tokens[idx]+tokens[idx+1]]+tokens[idx+2:]\n",
    "                    word_dict[word] = tokens\n",
    "                else:\n",
    "                    #if not merged then we slide the window over\n",
    "                    idx = idx+1\n",
    "        logging.info(f\"After merge {word_dict}\")\n",
    "    return word_dict    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977da63d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.518574Z",
     "iopub.status.busy": "2023-10-11T17:12:44.518223Z",
     "iopub.status.idle": "2023-10-11T17:12:44.524445Z",
     "shell.execute_reply": "2023-10-11T17:12:44.523261Z"
    },
    "papermill": {
     "duration": 0.016773,
     "end_time": "2023-10-11T17:12:44.526681",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.509908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'th'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_rules[('t', 'h')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb46066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.544765Z",
     "iopub.status.busy": "2023-10-11T17:12:44.544419Z",
     "iopub.status.idle": "2023-10-11T17:12:44.860731Z",
     "shell.execute_reply": "2023-10-11T17:12:44.859881Z"
    },
    "papermill": {
     "duration": 0.32615,
     "end_time": "2023-10-11T17:12:44.863441",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.537291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': ['this'], 'is': ['is'], 'an': ['an'], 'apple': ['apple']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sen1,merge_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9ebaf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.877219Z",
     "iopub.status.busy": "2023-10-11T17:12:44.876846Z",
     "iopub.status.idle": "2023-10-11T17:12:44.883250Z",
     "shell.execute_reply": "2023-10-11T17:12:44.881953Z"
    },
    "papermill": {
     "duration": 0.015561,
     "end_time": "2023-10-11T17:12:44.885164",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.869603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sen_to_tensor(sen,vocab,merge_rules):\n",
    "    '''Each sen will be converted to a vector where each token will be mapped to its index in vocab'''\n",
    "    sen = sen.lower()\n",
    "    token_dict = tokenize(sen,merge_rules)\n",
    "    words = nltk.word_tokenize(sen)\n",
    "    vocab = list(vocab)\n",
    "    list_of_tokens = []\n",
    "    for word in words:\n",
    "        list_of_tokens.extend(token_dict[word])\n",
    "    \n",
    "    #find the index of the token in the vocab\n",
    "    tensor = np.empty(len(list_of_tokens),dtype=int)\n",
    "    for idx,token in enumerate(list_of_tokens):\n",
    "        tensor[idx] = vocab.index(token)\n",
    "    return tensor\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c242d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:44.898612Z",
     "iopub.status.busy": "2023-10-11T17:12:44.898257Z",
     "iopub.status.idle": "2023-10-11T17:12:45.198495Z",
     "shell.execute_reply": "2023-10-11T17:12:45.197584Z"
    },
    "papermill": {
     "duration": 0.309333,
     "end_time": "2023-10-11T17:12:45.200611",
     "exception": false,
     "start_time": "2023-10-11T17:12:44.891278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3141,   34, 9520,   53])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_to_tensor(sen1,vocab,merge_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ff9161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:45.214672Z",
     "iopub.status.busy": "2023-10-11T17:12:45.214336Z",
     "iopub.status.idle": "2023-10-11T17:12:45.222343Z",
     "shell.execute_reply": "2023-10-11T17:12:45.221127Z"
    },
    "papermill": {
     "duration": 0.017804,
     "end_time": "2023-10-11T17:12:45.224597",
     "exception": false,
     "start_time": "2023-10-11T17:12:45.206793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_embed_sen(sen_tensor,n=100,d=4):\n",
    "    '''Given a sen tensor return its pos embed'''\n",
    "    seq_len = len(sen_tensor)\n",
    "    pos_embed_matrix = np.zeros((seq_len,d))\n",
    "    \n",
    "    for k in range(len(sen_tensor)):\n",
    "        for i in range(0,int(d/2)):\n",
    "            denom = np.power(n,((2*i)/d))\n",
    "            pos_embed_matrix[k,2*i] = np.sin(k/denom)#for even embed dim\n",
    "            pos_embed_matrix[k,2*i+1] = np.cos(k/denom)#for odd embed dim\n",
    "    \n",
    "    return pos_embed_matrix\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce7d77f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T17:12:45.238561Z",
     "iopub.status.busy": "2023-10-11T17:12:45.238219Z",
     "iopub.status.idle": "2023-10-11T17:12:45.556828Z",
     "shell.execute_reply": "2023-10-11T17:12:45.555923Z"
    },
    "papermill": {
     "duration": 0.328321,
     "end_time": "2023-10-11T17:12:45.559120",
     "exception": false,
     "start_time": "2023-10-11T17:12:45.230799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.84147098,  0.54030231,  0.09983342,  0.99500417],\n",
       "       [ 0.90929743, -0.41614684,  0.19866933,  0.98006658],\n",
       "       [ 0.14112001, -0.9899925 ,  0.29552021,  0.95533649]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor= sen_to_tensor(sen1,vocab,merge_rules)\n",
    "pos_embed_sen(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc43b16",
   "metadata": {
    "papermill": {
     "duration": 0.005987,
     "end_time": "2023-10-11T17:12:45.571739",
     "exception": false,
     "start_time": "2023-10-11T17:12:45.565752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.962717,
   "end_time": "2023-10-11T17:12:46.100075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-11T17:12:19.137358",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
